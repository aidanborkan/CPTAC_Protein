#AB CPTAC Version 2
#use pip install cptact priot to running 

#we can try a unified appraoch that collects transcriptomic and multiomic 
import cptac

def collect_data_for_cancer_types(cancer_types_map, data_sources, data_type):
    # Initialize the dictionary to hold all data
    all_data = {}

    # Loop through each cancer type using the mapping
    for short_name, long_name in cancer_types_map.items():
        try:
            # Initialize the cptac data for the given cancer type
            # Using the mapping to instantiate the correct object
            cancer_data = getattr(cptac, long_name)()
            data = {}

            # Get the sources for the specified data type (transcriptomics/proteomics)
            sources = data_sources.get(data_type, [])

            # Loop through each data source
            for source in sources:
                try:
                    # Use get_dataframe to fetch the data
                    df = cancer_data.get_dataframe(data_type, source=source)

                    # Rename columns with a dynamic suffix if collecting proteomics data
                    if data_type == "proteomics":
                        df = df.rename(columns={
                            col: f"{col[0]}_{short_name}_{source}_proteomics" 
                            if isinstance(col, tuple) and not col[0] in ['Patient_ID', 'Sample_ID'] else col
                            for col in df.columns
                        })

                    # Store the dataframe in the data dictionary
                    data[source] = {
                        'dataframe': df,
                        'index': df.index,
                        'columns': df.columns
                    }

                    print(f"Collected {data_type} data from {source} for {long_name}")
                except Exception as e:
                    print(f"Failed to collect {data_type} data from {source} for {long_name}: {e}")

            # Store the collected data for the current cancer type
            all_data[long_name] = data
        except Exception as e:
            print(f"Failed to initialize cptac for {long_name}: {e}")

    return all_data

# Define your cancer types and data sources using the provided map
cancer_types_map = {
    "en": "Ucec",
    "ov": "Ov",
    "co": "Coad",
    "br": "Brca",
    "lu": "Luad",
    "cc": "Ccrcc",
    "gb": "Gbm",
    "ls": "Lscc",
    "hn": "Hnscc"
}

data_sources = {
    "transcriptomics": ["bcm", "broad", "washu"],
    "proteomics": ["bcm", "umich"]
}

# Collect transcriptomics data
all_transcriptomics_data = collect_data_for_cancer_types(cancer_types_map, data_sources, "transcriptomics")

# Collect proteomics data
all_proteomics_data = collect_data_for_cancer_types(cancer_types_map, data_sources, "proteomics")v


cancer_types = ["Ucec", "Ov", "Coad", "Brca", "Luad", "Ccrcc", "Gbm", "Lscc", "Hnscc"]

#this function works to pull data by cancer type from our dictionaries
def get_data_for_cancer_type(cancer_type, all_data):
    """
    Retrieve data for a specific cancer type from a dictionary containing multiple cancer types' data.

    Parameters:
    - cancer_type (str): The type of cancer for which to retrieve data.
    - all_data (dict): A dictionary containing data for multiple cancer types.

    Returns:
    - dict: A dictionary containing data for the specified cancer type, organized by data source.
    """


    # Check if the specified cancer type exists in the dictionary
    if cancer_type in all_data:
        return all_data[cancer_type]
    else:
        print(f"No data found for {cancer_type}")
        return {}

# Example usage:
cancer_type_to_retrieve = 'Ucec'  # Specify the cancer type you want to retrieve data for

# Retrieve transcriptomics data for the specified cancer type
transcriptomics_data_for_cancer_type = get_data_for_cancer_type(cancer_type_to_retrieve, all_transcriptomics_data)

# Retrieve proteomics data for the specified cancer type
proteomics_data_for_cancer_type = get_data_for_cancer_type(cancer_type_to_retrieve, all_proteomics_data)

class DataContainer:
    def __init__(self):
        self.transcriptomics = None
        self.proteomics = None

# Mapping of short names to the corresponding cancer types
cancer_types_map = {
    "en": "Ucec",
    "ov": "Ov",
    "co": "Coad",
    "br": "Brca",
    "lu": "Luad",
    "cc": "Ccrcc",
    "gb": "Gbm",
    "ls": "Lscc",
    "hn": "Hnscc"
}

# Initialize a dictionary to hold the DataContainer objects
cancer_data_containers = {}

# Iterate over the mapping dictionary
for short_name, cancer_type in cancer_types_map.items():
    # Assume get_data_for_cancer_type is a function that retrieves the relevant data
    transcriptomics_data = get_data_for_cancer_type(cancer_type, all_transcriptomics_data)
    proteomics_data = get_data_for_cancer_type(cancer_type, all_proteomics_data)

    # Create a DataContainer instance and assign the retrieved data
    container = DataContainer()
    container.transcriptomics = transcriptomics_data
    container.proteomics = proteomics_data

    # Store the container in the dictionary with the short name as the key
    cancer_data_containers[short_name] = container

#Note that data from the Broad institute is stored differently and will not collect

def join_multiple_trans_with_prot(cancer_obj, trans_sources, prot_sources, var_name):
    # Initialize an empty DataFrame for storing combined transcriptomics data
    combined_trans = None

    # Iterate through the transcriptomics sources and join their data
    for trans_source in trans_sources:
        try:
            trans_data = cancer_obj.get_transcriptomics(source=trans_source)
            # Rename the columns to add '_transcriptomics' suffix, except for 'Name'
            trans_data = trans_data.rename(columns=lambda x: f"{x}_transcriptomics_{trans_source}" if x != 'Name' else x)

            # If combined_trans is not None, join new trans_data to it
            if combined_trans is not None:
                combined_trans = combined_trans.join(trans_data, how='outer')
            else:
                combined_trans = trans_data

        except Exception as e:
            print(f"Error getting transcriptomics data from {trans_source}: {e}")

    # Initialize an empty DataFrame for storing combined proteomics data
    combined_prot = None

    # Iterate through the proteomics sources and join their data
    for prot_source in prot_sources:
        try:
            prot_data = cancer_obj.get_proteomics(source=prot_source)
            # Rename the columns to add '_proteomics' suffix, except for 'Name'
            prot_data = prot_data.rename(columns=lambda x: f"{x}_proteomics_{prot_source}" if x != 'Name' else x)

            # If combined_prot is not None, join new prot_data to it
            if combined_prot is not None:
                combined_prot = combined_prot.join(prot_data, how='outer')
            else:
                combined_prot = prot_data

        except Exception as e:
            print(f"Error getting proteomics data from {prot_source}: {e}")

    # Join the combined transcriptomics data with the combined proteomics data
    try:
        joined_data = combined_trans.join(combined_prot, how='outer')

        # Assign the joined data to a variable named by the provided var_name
        globals()[var_name] = joined_data
        print(f"Joined data is available as variable '{var_name}'")

    except Exception as e:
        print(f"Error joining transcriptomics with proteomics data: {e}")

en = cptac.Ucec()
transcriptomics_sources_en = ['bcm', 'washu']
proteomics_sources_en = ['umich']  # Note the use of list even for single source
join_multiple_trans_with_prot(en, transcriptomics_sources_en, proteomics_sources_en, 'a')

ov = cptac.Ov()
transcriptomics_sources_ov = ['bcm', 'washu']
proteomics_sources_ov = ['bcm', 'umich']
join_multiple_trans_with_prot(ov, transcriptomics_sources_ov, proteomics_sources_ov, 'b')

ls = cptac.Lscc()
transcriptomics_sources_ls = ['bcm']
proteomics_sources_ls = ['bcm', 'umich']
join_multiple_trans_with_prot(ls, transcriptomics_sources_ls, proteomics_sources_ls, 'c')

co = cptac.Coad()
transcriptomics_sources_co = ['bcm', 'washu']
proteomics_sources_co = ['bcm', 'umich']
join_multiple_trans_with_prot(co, transcriptomics_sources_co, proteomics_sources_co, 'd')

br = cptac.Brca()
transcriptomics_sources_br = ['bcm', 'washu']
proteomics_sources_br = ['bcm', 'umich']
join_multiple_trans_with_prot(br, transcriptomics_sources_br, proteomics_sources_br, 'e')

lu = cptac.Luad()
transcriptomics_sources_lu = ['bcm', 'washu']
proteomics_sources_lu = ['bcm', 'umich']
join_multiple_trans_with_prot(lu, transcriptomics_sources_lu, proteomics_sources_lu, 'f')

cc = cptac.Ccrcc()
transcriptomics_sources_cc = ['bcm', 'washu']
proteomics_sources_cc = ['bcm', 'umich']
join_multiple_trans_with_prot(cc, transcriptomics_sources_cc, proteomics_sources_cc, 'g')

gb = cptac.Gbm()
transcriptomics_sources_gb = ['bcm', 'washu']
proteomics_sources_gb = ['bcm', 'umich']
join_multiple_trans_with_prot(gb, transcriptomics_sources_gb, proteomics_sources_gb, 'h')

hn = cptac.Hnscc()
transcriptomics_sources_hn = ['bcm', 'washu']
proteomics_sources_hn = ['bcm', 'umich']
join_multiple_trans_with_prot(hn, transcriptomics_sources_hn, proteomics_sources_hn, 'i')

from sklearn.preprocessing import StandardScaler
import numpy as np

def standardize_and_remove_duplicates(dataset_name):
    dataset = globals()[dataset_name]
    print(f"Processing {dataset_name}...")

    # Count columns before removing duplicates
    cols_before = dataset.shape[1]

    # Remove duplicated columns and keep the first occurrence
    dataset_no_dupes = dataset.loc[:, ~dataset.columns.duplicated(keep='first')]

    # Count columns after removing duplicates
    cols_after = dataset_no_dupes.shape[1]
    print(f"Columns before removing duplicates: {cols_before}")
    print(f"Columns after removing duplicates: {cols_after}")

    # Identify numeric columns
    numeric_cols = [col for col in dataset_no_dupes.columns if np.issubdtype(dataset_no_dupes[col].dtype, np.number)]

    # Standardize numeric columns
    if numeric_cols:
        scaler = StandardScaler()
        dataset_no_dupes[numeric_cols] = scaler.fit_transform(dataset_no_dupes[numeric_cols])

    # Store the processed dataset as a new variable with '_std' suffix
    globals()[f"{dataset_name}_std"] = dataset_no_dupes
    print(f"{dataset_name}_std created successfully.\n")

# List of dataset names to process
datasets_to_process = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']

# Iterate over the datasets and apply the function
for dataset_name in datasets_to_process:
    if dataset_name in globals():
        standardize_and_remove_duplicates(dataset_name)
    else:
        print(f"Dataset '{dataset_name}' not found in global namespace.")

#Save
from IPython.display import display

# Base names for the datasets
dataset_bases = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']

for base in dataset_bases:
    # Save original dataset
    original_name = base  # Original dataset name
    if original_name in globals():
        df_original = globals()[original_name]
        csv_original_name = f"{original_name}_original.csv"
        df_original.to_csv(csv_original_name, index=False)
        display(f"Saved original {original_name} to {csv_original_name}")
    else:
        display(f"Original dataset '{original_name}' not found in global namespace.")

    # Save standardized dataset
    std_name = f"{base}_std"  # Standardized dataset name
    if std_name in globals():
        df_std = globals()[std_name]
        csv_std_name = f"{std_name}.csv"
        df_std.to_csv(csv_std_name, index=False)
        display(f"Saved standardized {std_name} to {csv_std_name}")
    else:
        display(f"Standardized dataset '{std_name}' not found in global namespace.")
